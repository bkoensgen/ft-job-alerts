===== Arborescence du Projet =====

ft-job-alerts
├── .env
├── .env.example
├── .env_old
├── .gitignore
├── README.md
├── data
│   ├── ft_jobs.db
│   ├── out
│   │   ├── offres-20250912-185634.txt
│   │   ├── offres-20250912-185815.txt
│   │   └── offres-20250912-190355.txt
│   └── samples
│       ├── offres_detail
│       │   ├── OFFER123.json
│       │   └── OFFER456.json
│       └── offres_sample.json
├── run.py
└── src
    └── ft_job_alerts
        ├── __init__.py
        ├── auth.py
        ├── cli.py
        ├── clients.py
        ├── config.py
        ├── exporter.py
        ├── filters.py
        ├── notifier.py
        ├── scoring.py
        └── storage.py

===== Contenu des Fichiers =====



--- START OF FILE data/samples/offres_detail/OFFER123.json ---

{
  "id": "OFFER123",
  "description": "Nous recherchons un ingénieur robotique junior maîtrisant ROS 2, C++, vision (OpenCV) pour développer des fonctionnalités de navigation et perception. Participation à l'intégration capteurs (Lidar, caméras), SLAM, path planning. Connaissance de MoveIt et Gazebo appréciée.",
  "lienPostuler": "https://www.example.com/candidature/OFFER123",
  "salaire": {"libelle": "35-42k€"},
  "origineOffre": {"url": "https://www.example.com/offres/OFFER123"}
}



--- END OF FILE data/samples/offres_detail/OFFER123.json ---



--- START OF FILE data/samples/offres_detail/OFFER456.json ---

{
  "id": "OFFER456",
  "description": "Participation au développement ROS, ROS2 et drivers capteurs (Lidar, caméras). Environnement C++ moderne (C++17/20), CI/CD, MoveIt, Gazebo. Travail sur la navigation, perception, calibration et intégration matérielle.",
  "lienPostuler": "https://www.example.com/candidature/OFFER456",
  "salaire": {"libelle": "30-38k€"},
  "origineOffre": {"url": "https://www.example.com/offres/OFFER456"}
}



--- END OF FILE data/samples/offres_detail/OFFER456.json ---



--- START OF FILE data/samples/offres_sample.json ---

[
  {
    "id": "OFFER123",
    "intitule": "Ingénieur Robotique Junior (ROS2 / C++)",
    "description": "Nous recherchons un ingénieur robotique junior maîtrisant ROS 2, C++, vision (OpenCV) pour développer des fonctionnalités de navigation et perception.",
    "dateCreation": "2025-09-10T09:00:00Z",
    "typeContrat": "CDI",
    "entreprise": {"nom": "Robotics Alsace"},
    "lieuTravail": {"libelle": "Mulhouse", "departement": "68", "latitude": 47.75, "longitude": 7.34},
    "origineOffre": {"url": "https://www.example.com/offres/OFFER123"},
    "salaire": {"libelle": "35-42k€"}
  },
  {
    "id": "OFFER456",
    "intitule": "Développeur Logiciel Embarqué C++/ROS",
    "description": "Participation au développement ROS, ROS2 et drivers capteurs (Lidar, caméras). Environnement C++ moderne, CI/CD, MoveIt, Gazebo.",
    "dateCreation": "2025-09-11T10:30:00Z",
    "typeContrat": "CDD",
    "entreprise": {"nom": "AutonoTech"},
    "lieuTravail": {"libelle": "Colmar", "departement": "68", "latitude": 48.08, "longitude": 7.36},
    "origineOffre": {"url": "https://www.example.com/offres/OFFER456"},
    "salaire": {"libelle": "30-38k€"}
  },
  {
    "id": "OFFER789",
    "intitule": "Technico-commercial solutions robotisées",
    "description": "Prospection commerciale, suivi client. Connaissance du marché robotique appréciée.",
    "dateCreation": "2025-09-12T08:00:00Z",
    "typeContrat": "CDI",
    "entreprise": {"nom": "SalesBots"},
    "lieuTravail": {"libelle": "Strasbourg", "departement": "67", "latitude": 48.58, "longitude": 7.75},
    "origineOffre": {"url": "https://www.example.com/offres/OFFER789"}
  }
]



--- END OF FILE data/samples/offres_sample.json ---



--- START OF FILE run.py ---

import sys


def main():
    # Allow running without installing the package
    sys.path.insert(0, "src")
    from ft_job_alerts.cli import main as cli_main

    cli_main()


if __name__ == "__main__":
    main()



--- END OF FILE run.py ---



--- START OF FILE src/ft_job_alerts/__init__.py ---

__all__ = []



--- END OF FILE src/ft_job_alerts/__init__.py ---



--- START OF FILE src/ft_job_alerts/auth.py ---

from __future__ import annotations

import base64
import json
import time
import urllib.parse
import urllib.request
from urllib.error import HTTPError
from dataclasses import dataclass

from .config import Config


@dataclass
class Token:
    access_token: str
    expires_at: float

    def valid(self) -> bool:
        # small safety margin
        return time.time() < self.expires_at - 30


class AuthClient:
    def __init__(self, cfg: Config):
        self.cfg = cfg
        self._cached: Token | None = None

    def get_token(self) -> str:
        if self.cfg.api_simulate:
            # In simulate mode we just return a dummy token
            return "SIMULATED_TOKEN"
        if self._cached and self._cached.valid():
            return self._cached.access_token
        if not self.cfg.client_id or not self.cfg.client_secret:
            raise RuntimeError("Missing FT_CLIENT_ID / FT_CLIENT_SECRET for real API calls")
        token = self._fetch_token()
        self._cached = token
        return token.access_token

    def _fetch_token(self) -> Token:
        # Build payload according to FT partner OAuth requirements.
        # Most habilitations require a scope like: "application_{client_id} api_offresdemploiv2"
        scope = self.cfg.oauth_scope
        if not scope:
            # According to Offres v2 OpenAPI, accepted scopes include these two.
            scope = "api_offresdemploiv2 o2dsoffre"
        payload = {
            "grant_type": "client_credentials",
            "client_id": self.cfg.client_id,
            "client_secret": self.cfg.client_secret,
            "scope": scope,
        }
        if self.cfg.oauth_audience:
            payload["audience"] = self.cfg.oauth_audience
        data = urllib.parse.urlencode(payload).encode("utf-8")
        req = urllib.request.Request(self.cfg.auth_url, data=data)
        # Some auth endpoints require Basic auth. Make it configurable.
        if self.cfg.oauth_use_basic:
            basic = base64.b64encode(f"{self.cfg.client_id}:{self.cfg.client_secret}".encode()).decode()
            req.add_header("Authorization", f"Basic {basic}")
        req.add_header("Content-Type", "application/x-www-form-urlencoded")

        try:
            with urllib.request.urlopen(req, timeout=20) as resp:
                raw = resp.read().decode("utf-8")
        except HTTPError as e:
            try:
                err_body = e.read().decode("utf-8")
            except Exception:
                err_body = ""
            raise RuntimeError(f"OAuth token request failed ({e.code}): {err_body}")

        obj = json.loads(raw)
        access = obj.get("access_token")
        ttl = obj.get("expires_in", 3600)
        if not access:
            raise RuntimeError("No access_token in OAuth response")
        return Token(access_token=access, expires_at=time.time() + float(ttl))


--- END OF FILE src/ft_job_alerts/auth.py ---



--- START OF FILE src/ft_job_alerts/cli.py ---

from __future__ import annotations

import argparse
import datetime as dt
from typing import Any

from .auth import AuthClient
from .clients import OffresEmploiClient, ROMEClient
from .config import load_config
from .filters import is_relevant
from .notifier import format_offers, notify
from .scoring import score_offer
from .storage import due_followups, init_db, recent_new_offers, upsert_offers, mark_notified, query_offers
from .exporter import export_txt, export_md, export_csv, export_jsonl
from .storage import update_offer_details


def normalize_offer(o: dict[str, Any]) -> dict[str, Any]:
    # Normalize a raw API offer into our storage format
    oid = str(o.get("id") or o.get("offerId") or o.get("reference") or o.get("idOffre") or "")
    title = o.get("intitule") or o.get("title") or ""
    company = (o.get("entreprise") or {}).get("nom") if isinstance(o.get("entreprise"), dict) else (o.get("company") or "")
    city = ""
    department = ""
    postal_code = ""
    latitude = None
    longitude = None
    location = None
    if isinstance(o.get("lieuTravail"), dict):
        lt = o["lieuTravail"]
        city = lt.get("libelle") or lt.get("ville") or ""
        department = lt.get("departement") or lt.get("codePostal", "")[:2]
        postal_code = lt.get("codePostal") or ""
        latitude = lt.get("latitude")
        longitude = lt.get("longitude")
        location = f"{city} ({department})".strip()
    else:
        location = o.get("location") or ""

    contract = o.get("typeContrat") or o.get("contractType") or ""
    published = o.get("dateCreation") or o.get("publishedAt") or o.get("publication") or ""
    url = o.get("origineOffre", {}).get("url") if isinstance(o.get("origineOffre"), dict) else (o.get("url") or "")
    apply_url = o.get("lienPostuler") or url
    salary = o.get("salaire", {}).get("libelle") if isinstance(o.get("salaire"), dict) else (o.get("salary") or "")
    description = o.get("description") or ""

    return {
        "offer_id": oid,
        "title": title,
        "company": company or "",
        "location": location or "",
        "contract_type": contract or "",
        "published_at": published or "",
        "url": url or "",
        "apply_url": apply_url or "",
        "salary": salary or "",
        "description": description or "",
        "city": city or "",
        "department": department or "",
        "postal_code": postal_code or "",
        "latitude": latitude,
        "longitude": longitude,
        # filled later
        "rome_codes": [],
        "keywords": [],
        "score": 0.0,
    }


def cmd_init_db(_args):
    init_db()
    print("DB initialized at data/ft_jobs.db")


def cmd_set_status(args):
    from .storage import set_status

    set_status(args.offer_id, args.status)
    print(f"Offer {args.offer_id} set to {args.status}")


def cmd_fetch(args):
    cfg = load_config()
    auth = AuthClient(cfg)
    client = OffresEmploiClient(cfg, auth)
    rome = ROMEClient(cfg)

    if cfg.api_simulate:
        print("[info] FT_API_SIMULATE=1 → utilisation des données d'exemple locales (pas d'appel réseau)")
        print("       Mettez FT_API_SIMULATE=0 + FT_CLIENT_ID/FT_CLIENT_SECRET pour interroger l'API réelle.")

    keywords = [k.strip() for k in args.keywords.split(",") if k.strip()] if args.keywords else cfg.default_keywords
    rome_codes = []
    if args.rome:
        rome_codes = [r.strip() for r in args.rome.split(",") if r.strip()]
    elif args.auto_rome:
        rome_codes = rome.map_keywords_to_rome(keywords)

    # Location parameters
    departements = None
    if args.dept:
        departements = [d.strip() for d in str(args.dept).split(",") if d.strip()]
    commune = args.commune
    # `distance` only applies if `commune` is set. Keep radius_km for backward compat.
    distance_km = args.distance_km if args.distance_km is not None else args.radius_km
    if distance_km is not None and not commune:
        print("[warn] --distance-km/--radius-km est ignoré si --commune n'est pas fourni (API FT)")

    raw = client.search(
        keywords=keywords,
        departements=departements,
        commune=commune,
        distance_km=distance_km,
        rome_codes=rome_codes,
        limit=args.limit,
        page=args.page,
        sort=args.sort,
        published_since_days=args.published_since_days,
    )

    # Filter + score
    base_lat = 47.76
    base_lon = 7.34
    prepared: list[dict[str, Any]] = []
    for r in raw:
        n = normalize_offer(r)
        if not n["offer_id"]:
            continue
        if not is_relevant(n["title"], n.get("description")):
            continue
        n["rome_codes"] = rome_codes
        n["keywords"] = keywords
        n["score"] = score_offer(r, base_lat=base_lat, base_lon=base_lon)
        try:
            import json as _json
            n["raw_json"] = _json.dumps(r, ensure_ascii=False)
        except Exception:
            n["raw_json"] = None
        prepared.append(n)

    inserted = upsert_offers(prepared)
    print(f"Prepared: {len(prepared)} offers; inserted/updated: {inserted}")


def cmd_run_daily(args):
    # Fetch new + notify new + notify follow-ups
    cmd_fetch(args)
    cfg = load_config()
    new_rows = recent_new_offers(limit=30)
    fu_rows = due_followups()
    subject = "FT Job Alerts — New & Follow-ups"
    body_parts = []
    if new_rows:
        body_parts.append("New offers:\n" + format_offers(new_rows))
    if fu_rows:
        body_parts.append("\nDue follow-ups:\n" + format_offers(fu_rows))
    if not body_parts:
        body_parts.append("No new offers or follow-ups today.")
    notify(cfg, subject, "\n\n".join(body_parts))
    # Mark newly notified offers to avoid duplicate alerts next runs
    if new_rows:
        mark_notified([r["offer_id"] for r in new_rows])


def cmd_export(args):
    # Query rows based on filters, export in chosen format
    rows = query_offers(
        days=args.days,
        from_date=args.from_date,
        to_date=args.to_date,
        status=args.status,
        min_score=args.min_score,
        limit=args.limit,
        order_by="score_desc",
    )
    if args.format == "txt":
        path = export_txt(rows, args.outfile, desc_chars=args.desc_chars)
    elif args.format == "md":
        path = export_md(rows, args.outfile, desc_chars=args.desc_chars)
    elif args.format == "csv":
        path = export_csv(rows, args.outfile)
    else:
        path = export_jsonl(rows, args.outfile)
    print(f"Exported {len(rows)} rows to {path}")


def extract_detail_fields(detail: dict[str, Any]) -> dict[str, Any]:
    # Be tolerant to varying schemas; keep the essentials.
    def _get(d: dict, path: list[str], default=None):
        cur = d
        for k in path:
            if not isinstance(cur, dict):
                return default
            cur = cur.get(k)
        return cur if cur is not None else default

    description = detail.get("description") or _get(detail, ["descriptionOffre"]) or ""
    # Try various keys that may contain an application URL
    apply_url = (
        detail.get("lienPostuler")
        or _get(detail, ["contact", "urlCandidature"])  # example in some schemas
        or _get(detail, ["origineOffre", "url"])
        or detail.get("url")
        or ""
    )
    url = _get(detail, ["origineOffre", "url"]) or detail.get("url") or ""
    salary = _get(detail, ["salaire", "libelle"]) or detail.get("salaire") or ""
    import json as _json
    return {
        "description": description,
        "apply_url": apply_url,
        "url": url,
        "salary": salary,
        "raw_json": _json.dumps(detail, ensure_ascii=False),
    }


def cmd_enrich(args):
    cfg = load_config()
    auth = AuthClient(cfg)
    client = OffresEmploiClient(cfg, auth)

    ids: list[str]
    if args.ids:
        ids = [s.strip() for s in args.ids.split(",") if s.strip()]
    else:
        rows = query_offers(
            days=args.days,
            from_date=args.from_date,
            to_date=args.to_date,
            status=args.status,
            min_score=args.min_score,
            limit=args.limit,
            order_by="date_desc",
        )
        # If only missing description requested, filter here
        if args.only_missing_description:
            rows = [r for r in rows if not (r["description"] and len(str(r["description"]).strip()) >= args.min_desc_len)]
        ids = [r["offer_id"] for r in rows]

    import time
    updated = 0
    for oid in ids:
        try:
            d = client.detail(oid)
            if not d:
                continue
            fields = extract_detail_fields(d)
            # Skip if no new info
            if not any(fields.get(k) for k in ("description", "apply_url", "salary")):
                continue
            update_offer_details(oid, fields)
            updated += 1
            time.sleep(max(0, args.sleep_ms) / 1000.0)
        except Exception as e:
            print(f"enrich: failed {oid}: {e}")
    print(f"Enriched {updated} offers (from {len(ids)})")


def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="ft-job-alerts", description="France Travail job alerts pipeline")
    sub = p.add_subparsers(dest="cmd", required=True)

    s_init = sub.add_parser("init-db", help="Initialize SQLite DB")
    s_init.set_defaults(func=cmd_init_db)

    s_status = sub.add_parser("set-status", help="Update offer status (e.g., applied)")
    s_status.add_argument("--offer-id", required=True)
    s_status.add_argument("--status", required=True, choices=["new", "applied", "rejected", "to_follow"])
    s_status.set_defaults(func=cmd_set_status)

    s_fetch = sub.add_parser("fetch", help="Fetch (or simulate) and store offers")
    s_fetch.add_argument("--keywords", default=None, help="Comma-separated keywords (default from env)")
    s_fetch.add_argument("--rome", default=None, help="Comma-separated ROME codes (override)")
    s_fetch.add_argument("--auto-rome", action="store_true", help="Derive ROME codes from keywords (stub)")
    s_fetch.add_argument("--dept", default=None, help="Department code(s), comma-separated (e.g., 68 or 68,67)")
    s_fetch.add_argument("--commune", default=None, help="INSEE commune code (required to use --distance-km)")
    s_fetch.add_argument("--distance-km", type=int, default=None, help="Radius in km around --commune (API param distance)")
    s_fetch.add_argument("--radius-km", type=int, default=None, help="Deprecated alias for --distance-km")
    s_fetch.add_argument("--limit", type=int, default=50)
    s_fetch.add_argument("--page", type=int, default=0, help="Page index (multiplies the range window)")
    s_fetch.add_argument("--sort", type=int, choices=[0, 1, 2], default=1,
                         help="0=pertinence/date, 1=date/pertinence, 2=distance/pertinence")
    s_fetch.add_argument("--published-since-days", dest="published_since_days", type=int, default=None,
                         help="Only offers published since N days (if supported by API)")
    s_fetch.set_defaults(func=cmd_fetch)

    s_run = sub.add_parser("run-daily", help="Fetch + notify new and follow-ups")
    s_run.add_argument("--keywords", default=None)
    s_run.add_argument("--rome", default=None)
    s_run.add_argument("--auto-rome", action="store_true")
    s_run.add_argument("--dept", default=None)
    s_run.add_argument("--commune", default=None)
    s_run.add_argument("--distance-km", type=int, default=None)
    s_run.add_argument("--radius-km", type=int, default=None)
    s_run.add_argument("--limit", type=int, default=50)
    s_run.add_argument("--page", type=int, default=0)
    s_run.add_argument("--sort", type=int, choices=[0, 1, 2], default=1)
    s_run.add_argument("--published-since-days", dest="published_since_days", type=int, default=1)
    s_run.set_defaults(func=cmd_run_daily)

    s_export = sub.add_parser("export", help="Export offers (txt/csv/md/jsonl) for analysis")
    s_export.add_argument("--format", choices=["txt", "csv", "md", "jsonl"], default="txt")
    s_export.add_argument("--days", type=int, default=None, help="Window on inserted_at (last N days)")
    s_export.add_argument("--from", dest="from_date", default=None, help="From date YYYY-MM-DD (inserted_at)")
    s_export.add_argument("--to", dest="to_date", default=None, help="To date YYYY-MM-DD (inserted_at)")
    s_export.add_argument("--status", default=None, help="Filter by status (new,applied,rejected,to_follow)")
    s_export.add_argument("--min-score", dest="min_score", type=float, default=None)
    s_export.add_argument("--top", dest="limit", type=int, default=100)
    s_export.add_argument("--outfile", default=None, help="Output path; defaults to data/out/…")
    s_export.add_argument("--desc-chars", dest="desc_chars", type=int, default=400,
                         help="Include description truncated to N chars (0 to omit)")
    s_export.set_defaults(func=cmd_export)

    s_enrich = sub.add_parser("enrich", help="Fetch offer details to fill full description/apply URL/salary")
    s_enrich.add_argument("--ids", default=None, help="Comma-separated offer IDs to enrich (overrides filters)")
    s_enrich.add_argument("--days", type=int, default=None)
    s_enrich.add_argument("--from", dest="from_date", default=None)
    s_enrich.add_argument("--to", dest="to_date", default=None)
    s_enrich.add_argument("--status", default=None)
    s_enrich.add_argument("--min-score", dest="min_score", type=float, default=None)
    s_enrich.add_argument("--only-missing-description", action="store_true",
                          help="Only enrich offers with empty or very short description")
    s_enrich.add_argument("--min-desc-len", type=int, default=40,
                          help="Threshold for considering a description as present")
    s_enrich.add_argument("--limit", type=int, default=100, help="Max number of offers to enrich")
    s_enrich.add_argument("--sleep-ms", type=int, default=250, help="Delay between calls (rate limit)")
    s_enrich.set_defaults(func=cmd_enrich)

    s_auth = sub.add_parser("auth-check", help="Check OAuth config and attempt to fetch a token (redacted output)")
    s_auth.set_defaults(func=cmd_auth_check)

    return p


def main(argv: list[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(argv)
    # Dispatch including auth-check defined below
    args.func(args)


def cmd_auth_check(_args):
    cfg = load_config()
    redacted_id = (cfg.client_id[:6] + "…" + cfg.client_id[-4:]) if cfg.client_id else None
    print("Auth endpoint:", cfg.auth_url)
    print("Client ID:", redacted_id)
    print("Use Basic:", cfg.oauth_use_basic)
    print("Scope:", cfg.oauth_scope or "api_offresdemploiv2 o2dsoffre (default)")
    try:
        token = AuthClient(cfg).get_token()
        print("Token OK (length):", len(token))
    except Exception as e:
        print("Auth error:", e)


if __name__ == "__main__":
    main()


--- END OF FILE src/ft_job_alerts/cli.py ---



--- START OF FILE src/ft_job_alerts/clients.py ---

from __future__ import annotations

import json
import os
import pathlib
import urllib.parse
import urllib.request
from typing import Any

from .auth import AuthClient
from .config import Config


class OffresEmploiClient:
    def __init__(self, cfg: Config, auth: AuthClient):
        self.cfg = cfg
        self.auth = auth

    def search(
        self,
        *,
        keywords: list[str],
        departements: list[str] | None = None,
        commune: str | None = None,
        distance_km: int | None = None,
        rome_codes: list[str] | None = None,
        limit: int = 50,
        page: int = 0,
        sort: int | None = None,
        published_since_days: int | None = None,
    ) -> list[dict[str, Any]]:
        if self.cfg.api_simulate:
            return self._load_sample()

        params: dict[str, Any] = {}
        if keywords:
            params["motsCles"] = ",".join(keywords)
        if departements:
            params["departement"] = ",".join(departements)
        if commune:
            params["commune"] = commune
            if distance_km is not None:
                params["distance"] = int(distance_km)
        if rome_codes:
            params["codeROME"] = ",".join(rome_codes)
        if sort is not None:
            params["sort"] = int(sort)
        if published_since_days is not None:
            params["publieeDepuis"] = int(published_since_days)

        # Pagination via range (0-based)
        lim = min(max(int(limit), 1), 150)
        start = max(int(page), 0) * lim
        end = start + lim - 1
        params["range"] = f"{start}-{end}"

        url = f"{self.cfg.offres_search_url}?{urllib.parse.urlencode(params)}"
        token = self.auth.get_token()
        req = urllib.request.Request(url)
        req.add_header("Authorization", f"Bearer {token}")
        req.add_header("Accept", "application/json")
        with urllib.request.urlopen(req, timeout=20) as resp:
            raw = resp.read().decode("utf-8")
        obj = json.loads(raw) if raw else {}
        # The exact structure may vary; normalize to a list of offers
        if isinstance(obj, dict) and "resultats" in obj:
            return obj["resultats"]
        if isinstance(obj, list):
            return obj
        return []

    def _load_sample(self) -> list[dict[str, Any]]:
        sample_path = pathlib.Path("data/samples/offres_sample.json")
        if not sample_path.exists():
            return []
        with open(sample_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def detail(self, offer_id: str) -> dict[str, Any]:
        if self.cfg.api_simulate:
            path = pathlib.Path("data/samples/offres_detail") / f"{offer_id}.json"
            if path.exists():
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            return {}
        url = self.cfg.offres_detail_url.replace("{id}", urllib.parse.quote(str(offer_id)))
        token = self.auth.get_token()
        req = urllib.request.Request(url)
        req.add_header("Authorization", f"Bearer {token}")
        req.add_header("Accept", "application/json")
        with urllib.request.urlopen(req, timeout=20) as resp:
            raw = resp.read().decode("utf-8")
        return json.loads(raw)


class ROMEClient:
    def __init__(self, cfg: Config):
        self.cfg = cfg

    def map_keywords_to_rome(self, keywords: list[str]) -> list[str]:
        # Stub: provide a very small hand-mapping for robotics-ish keywords.
        # Prefer providing ROME codes manually or use ROMEO v2 later.
        mapping = {
            "ros": ["I1401"],     # placeholder ROME codes; adjust to your need
            "ros2": ["I1401"],
            "robot": ["H1203", "I1401"],
            "vision": ["I1308"],
            "c++": ["M1805"],
        }
        out: set[str] = set()
        for k in keywords:
            k0 = k.lower().strip()
            for key, codes in mapping.items():
                if key in k0:
                    out.update(codes)
        return sorted(out)


class LBBClient:
    def __init__(self, cfg: Config):
        self.cfg = cfg

    def top_companies(self, *, rome_codes: list[str], dept: str, limit: int = 20) -> list[dict[str, Any]]:
        # Stub: return an empty list in simulate mode; integrate later with real API
        return []


--- END OF FILE src/ft_job_alerts/clients.py ---



--- START OF FILE src/ft_job_alerts/config.py ---

import os
from dataclasses import dataclass


def _get_bool(name: str, default: bool = False) -> bool:
    val = os.getenv(name)
    if val is None:
        return default
    return str(val).strip() not in ("0", "false", "FALSE", "no", "No", "")


def _load_dotenv_if_present() -> None:
    path = os.path.join(os.getcwd(), ".env")
    if not os.path.exists(path):
        return
    try:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if "=" not in line:
                    continue
                key, val = line.split("=", 1)
                key = key.strip()
                val = val.strip().strip('"').strip("'")
                if key and key not in os.environ:
                    os.environ[key] = val
    except Exception:
        # Silently ignore .env parsing errors; user can export manually
        pass


@dataclass
class Config:
    api_simulate: bool
    client_id: str | None
    client_secret: str | None
    auth_url: str
    oauth_scope: str | None
    oauth_audience: str | None
    oauth_use_basic: bool
    offres_search_url: str
    offres_detail_url: str
    lbb_api_url: str | None
    rome_api_url: str | None
    email_to: str | None
    smtp_host: str | None
    smtp_port: int
    smtp_user: str | None
    smtp_password: str | None
    smtp_starttls: bool
    default_keywords: list[str]
    default_dept: str | None
    default_radius_km: int


def load_config() -> Config:
    _load_dotenv_if_present()
    return Config(
        api_simulate=_get_bool("FT_API_SIMULATE", True),
        client_id=os.getenv("FT_CLIENT_ID"),
        client_secret=os.getenv("FT_CLIENT_SECRET"),
        auth_url=os.getenv(
            "FT_AUTH_URL",
            "https://entreprise.francetravail.fr/connexion/oauth2/access_token?realm=/partenaire",
        ),
        oauth_scope=os.getenv("FT_SCOPE"),
        oauth_audience=os.getenv("FT_AUDIENCE"),
        oauth_use_basic=_get_bool("FT_OAUTH_BASIC", True),
        offres_search_url=os.getenv(
            "FT_OFFRES_SEARCH_URL",
            "https://api.francetravail.io/partenaire/offresdemploi/v2/offres/search",
        ),
        offres_detail_url=os.getenv(
            "FT_OFFRES_DETAIL_URL",
            "https://api.francetravail.io/partenaire/offresdemploi/v2/offres/{id}",
        ),
        lbb_api_url=os.getenv("LBB_API_URL"),
        rome_api_url=os.getenv("ROME_API_URL"),
        email_to=os.getenv("EMAIL_TO"),
        smtp_host=os.getenv("SMTP_HOST"),
        smtp_port=int(os.getenv("SMTP_PORT", "587")),
        smtp_user=os.getenv("SMTP_USER"),
        smtp_password=os.getenv("SMTP_PASSWORD"),
        smtp_starttls=_get_bool("SMTP_STARTTLS", True),
        default_keywords=[k.strip() for k in os.getenv("DEFAULT_KEYWORDS", "ros2,c++,vision").split(",") if k.strip()],
        default_dept=os.getenv("DEFAULT_DEPT", "68"),
        default_radius_km=int(os.getenv("DEFAULT_RADIUS_KM", "50")),
    )


--- END OF FILE src/ft_job_alerts/config.py ---



--- START OF FILE src/ft_job_alerts/exporter.py ---

from __future__ import annotations

import csv
import datetime as dt
import os
from typing import Iterable


def _ensure_out_dir() -> str:
    os.makedirs("data/out", exist_ok=True)
    return "data/out"


def export_txt(rows, outfile: str | None = None, desc_chars: int | None = 400) -> str:
    outdir = _ensure_out_dir()
    if not outfile:
        ts = dt.datetime.now().strftime("%Y%m%d-%H%M%S")
        outfile = os.path.join(outdir, f"offres-{ts}.txt")
    with open(outfile, "w", encoding="utf-8") as f:
        for r in rows:
            desc = r["description"] or ""
            if desc_chars == 0:
                desc = ""
            elif desc_chars is not None and desc_chars > 0 and len(desc) > desc_chars:
                desc = desc[: desc_chars].rstrip() + "…"
            loc_detail = f"{r['city']} ({r['department']})" if r["city"] else r["location"]
            line1 = (
                f"- [{r['score']:.2f}] {r['title']} — {r['company']} — {loc_detail} — {r['contract_type']} — {r['published_at']}\n"
                f"  ID: {r['offer_id']}\n  URL: {r['url']}\n"
            )
            f.write(line1)
            if desc:
                f.write(f"  Desc: {desc}\n")
            f.write("\n")
    return outfile


def export_md(rows, outfile: str | None = None, desc_chars: int | None = 500) -> str:
    outdir = _ensure_out_dir()
    if not outfile:
        ts = dt.datetime.now().strftime("%Y%m%d-%H%M%S")
        outfile = os.path.join(outdir, f"offres-{ts}.md")
    with open(outfile, "w", encoding="utf-8") as f:
        f.write("# Offres (sélection)\n\n")
        for r in rows:
            loc_detail = f"{r['city']} ({r['department']})" if r["city"] else r["location"]
            f.write(f"## {r['title']} ({r['score']:.2f})\n")
            f.write(f"- Entreprise: {r['company']}\n")
            f.write(f"- Lieu: {loc_detail}\n")
            f.write(f"- Contrat: {r['contract_type']}\n")
            f.write(f"- Publiée: {r['published_at']}\n")
            f.write(f"- ID: `{r['offer_id']}`\n")
            f.write(f"- URL: {r['url']}\n")
            desc = r["description"] or ""
            if desc_chars == 0:
                desc = ""
            if desc:
                if desc_chars is not None and desc_chars > 0 and len(desc) > desc_chars:
                    desc = desc[: desc_chars].rstrip() + "…"
                f.write("\n" + desc + "\n")
            f.write("\n")
    return outfile


def export_csv(rows, outfile: str | None = None) -> str:
    outdir = _ensure_out_dir()
    if not outfile:
        ts = dt.datetime.now().strftime("%Y%m%d-%H%M%S")
        outfile = os.path.join(outdir, f"offres-{ts}.csv")
    fieldnames = [
        "offer_id",
        "title",
        "company",
        "location",
        "city",
        "department",
        "postal_code",
        "latitude",
        "longitude",
        "contract_type",
        "published_at",
        "source",
        "url",
        "apply_url",
        "salary",
        "score",
        "status",
        "inserted_at",
        "rome_codes",
        "keywords",
        "description",
    ]
    with open(outfile, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        w.writeheader()
        for r in rows:
            w.writerow({k: r[k] for k in fieldnames})
    return outfile


def export_jsonl(rows, outfile: str | None = None) -> str:
    outdir = _ensure_out_dir()
    if not outfile:
        ts = dt.datetime.now().strftime("%Y%m%d-%H%M%S")
        outfile = os.path.join(outdir, f"offres-{ts}.jsonl")
    import json
    with open(outfile, "w", encoding="utf-8") as f:
        for r in rows:
            obj = {k: r[k] for k in r.keys()}
            # Keep raw_json as is; consumers can parse if needed
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
    return outfile


--- END OF FILE src/ft_job_alerts/exporter.py ---



--- START OF FILE src/ft_job_alerts/filters.py ---

from __future__ import annotations

import re
from typing import Iterable


# Basic keyword filters for robotics/ROS2/C++/vision oriented queries.
MUST_ANY = [
    r"\bros ?2\b",
    r"\bros2\b",
    r"\bros\b",
    r"\brobot(?:ique|ics)?\b",
    r"\bvision\b",
    r"\bc\+\+\b",
    r"\bperception\b",
    r"\bnavigation\b",
    r"\bslam\b",
    r"\bopencv\b",
    r"\bmoveit\b",
]

EXCLUDE = [
    r"\bcommercial\b",
    r"\btechnico-commercial\b",
    r"\bvendeur\b",
    r"\bchauffeur\b",
    r"\bserveur\b",
    r"\blogistique\b",
]


def _match_any(patterns: Iterable[str], text: str) -> bool:
    for p in patterns:
        if re.search(p, text, flags=re.IGNORECASE):
            return True
    return False


def is_relevant(title: str, description: str | None) -> bool:
    text = f"{title}\n{description or ''}"
    if not _match_any(MUST_ANY, text):
        return False
    if _match_any(EXCLUDE, text):
        return False
    return True



--- END OF FILE src/ft_job_alerts/filters.py ---



--- START OF FILE src/ft_job_alerts/notifier.py ---

from __future__ import annotations

import datetime as dt
import os
import smtplib
from email.message import EmailMessage
from typing import Iterable

from .config import Config


def format_offers(rows) -> str:
    lines = []
    for r in rows:
        line = f"[{r['score']:.2f}] {r['title']} — {r['company']} — {r['location']} — {r['url']}"
        lines.append(line)
    return "\n".join(lines)


def notify(cfg: Config, subject: str, body: str) -> None:
    if cfg.email_to and cfg.smtp_host:
        _send_email(cfg, subject, body)
    else:
        _write_file_and_print(subject, body)


def _send_email(cfg: Config, subject: str, body: str) -> None:
    msg = EmailMessage()
    msg["Subject"] = subject
    msg["From"] = cfg.smtp_user or "ft-job-alerts"
    msg["To"] = cfg.email_to
    msg.set_content(body)

    with smtplib.SMTP(cfg.smtp_host, cfg.smtp_port, timeout=10) as server:
        if cfg.smtp_starttls:
            server.starttls()
        if cfg.smtp_user and cfg.smtp_password:
            server.login(cfg.smtp_user, cfg.smtp_password)
        server.send_message(msg)


def _write_file_and_print(subject: str, body: str) -> None:
    os.makedirs("data/out", exist_ok=True)
    ts = dt.datetime.now().strftime("%Y%m%d-%H%M%S")
    path = os.path.join("data/out", f"notification-{ts}.txt")
    content = subject + "\n\n" + body
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)
    print("\n=== Notification ===")
    print(content)
    print(f"Saved to {path}")



--- END OF FILE src/ft_job_alerts/notifier.py ---



--- START OF FILE src/ft_job_alerts/scoring.py ---

from __future__ import annotations

import math
import re
from typing import Any


KEYWORD_WEIGHTS: list[tuple[str, float]] = [
    (r"\bros ?2\b|\bros2\b", 3.0),
    (r"\bc\+\+\b|\bcpp\b", 2.5),
    (r"\bvision\b|\bperception\b|\bopencv\b", 1.5),
    (r"\brobot(?:ique|ics)?\b|\bmoveit\b|\burgdf\b|\bgazebo\b|\bisaac\b", 2.0),
    (r"\bslam\b|\bnavigation\b|\bpath planning\b", 1.7),
]


def score_offer(offer: dict[str, Any], *, base_lat: float | None = None, base_lon: float | None = None) -> float:
    text = " ".join(
        [
            str(offer.get("intitule", "")),
            str(offer.get("description", "")),
        ]
    )
    s = 0.0
    for pattern, w in KEYWORD_WEIGHTS:
        if re.search(pattern, text, flags=re.IGNORECASE):
            s += w

    # Contract preference: CDI ≥ CDD ≥ Alternance ≥ Stage
    contrat = str(offer.get("typeContrat", "")).upper()
    if "CDI" in contrat:
        s += 1.5
    elif "CDD" in contrat:
        s += 0.8
    elif "ALTERN" in contrat:
        s += 0.4
    elif "STAGE" in contrat:
        s += 0.3

    # Distance bonus if coordinates present and base provided
    if base_lat is not None and base_lon is not None:
        lat = None
        lon = None
        if isinstance(offer.get("lieuTravail"), dict):
            lat = offer.get("lieuTravail", {}).get("latitude")
            lon = offer.get("lieuTravail", {}).get("longitude")
        # If already normalized
        lat = offer.get("latitude", lat)
        lon = offer.get("longitude", lon)
        if lat is not None and lon is not None:
            d = haversine_km(float(base_lat), float(base_lon), float(lat), float(lon))
            if d <= 20:
                s += 1.5
            elif d <= 50:
                s += 0.8
            elif d <= 100:
                s += 0.3
    return round(s, 3)


def haversine_km(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    R = 6371.0
    p1, p2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi / 2) ** 2 + math.cos(p1) * math.cos(p2) * math.sin(dlambda / 2) ** 2
    return 2 * R * math.asin(math.sqrt(a))


--- END OF FILE src/ft_job_alerts/scoring.py ---



--- START OF FILE src/ft_job_alerts/storage.py ---

from __future__ import annotations

import datetime as dt
import os
import sqlite3
from typing import Any, Iterable


DB_PATH = os.path.join("data", "ft_jobs.db")


def ensure_dirs() -> None:
    os.makedirs("data", exist_ok=True)
    os.makedirs("data/out", exist_ok=True)
    os.makedirs("data/samples", exist_ok=True)


def connect() -> sqlite3.Connection:
    ensure_dirs()
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn


def init_db() -> None:
    con = connect()
    cur = con.cursor()
    cur.executescript(
        """
        CREATE TABLE IF NOT EXISTS offers (
            offer_id TEXT PRIMARY KEY,
            title TEXT,
            company TEXT,
            location TEXT,
            city TEXT,
            department TEXT,
            postal_code TEXT,
            latitude REAL,
            longitude REAL,
            description TEXT,
            rome_codes TEXT,
            keywords TEXT,
            contract_type TEXT,
            published_at TEXT,
            source TEXT,
            url TEXT,
            apply_url TEXT,
            salary TEXT,
            score REAL DEFAULT 0,
            inserted_at TEXT,
            status TEXT DEFAULT 'new',
            followup1_due TEXT,
            followup2_due TEXT,
            last_notified_at TEXT,
            raw_json TEXT
        );

        CREATE INDEX IF NOT EXISTS idx_offers_status ON offers(status);
        CREATE INDEX IF NOT EXISTS idx_offers_published_at ON offers(published_at);
        """
    )
    # Migrations for older DBs: add columns if missing
    ensure_offer_columns(cur)
    con.commit()
    con.close()


def ensure_offer_columns(cur: sqlite3.Cursor) -> None:
    cur.execute("PRAGMA table_info(offers)")
    cols = {row[1] for row in cur.fetchall()}
    def add(col: str, sql_type: str):
        cur.execute(f"ALTER TABLE offers ADD COLUMN {col} {sql_type}")
    wanted = {
        "city": "TEXT",
        "department": "TEXT",
        "postal_code": "TEXT",
        "latitude": "REAL",
        "longitude": "REAL",
        "description": "TEXT",
        "apply_url": "TEXT",
        "raw_json": "TEXT",
    }
    for name, typ in wanted.items():
        if name not in cols:
            add(name, typ)


def upsert_offers(offers: Iterable[dict[str, Any]]) -> int:
    con = connect()
    cur = con.cursor()
    now = dt.datetime.utcnow().isoformat()
    inserted = 0
    for o in offers:
        cur.execute(
            """
            INSERT INTO offers (
                offer_id, title, company, location,
                city, department, postal_code, latitude, longitude,
                description, rome_codes, keywords,
                contract_type, published_at, source, url, apply_url, salary,
                score, inserted_at, raw_json
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(offer_id) DO UPDATE SET
                score=excluded.score,
                title=excluded.title,
                company=excluded.company,
                location=excluded.location,
                city=excluded.city,
                department=excluded.department,
                postal_code=excluded.postal_code,
                latitude=excluded.latitude,
                longitude=excluded.longitude,
                description=excluded.description,
                url=excluded.url,
                apply_url=excluded.apply_url,
                salary=excluded.salary
            ;
            """,
            (
                o.get("offer_id"),
                o.get("title"),
                o.get("company"),
                o.get("location"),
                o.get("city"),
                o.get("department"),
                o.get("postal_code"),
                o.get("latitude"),
                o.get("longitude"),
                o.get("description"),
                ",".join(o.get("rome_codes", [])),
                ",".join(o.get("keywords", [])),
                o.get("contract_type"),
                o.get("published_at"),
                o.get("source", "offres_v2"),
                o.get("url"),
                o.get("apply_url"),
                o.get("salary"),
                float(o.get("score", 0)),
                now,
                o.get("raw_json"),
            ),
        )
        if cur.rowcount == 1:
            inserted += 1
    con.commit()
    con.close()
    return inserted


def set_status(offer_id: str, status: str) -> None:
    con = connect()
    cur = con.cursor()
    fu1 = fu2 = None
    if status == "applied":
        d1 = dt.date.today() + dt.timedelta(days=5)
        d2 = dt.date.today() + dt.timedelta(days=12)
        fu1, fu2 = d1.isoformat(), d2.isoformat()
    cur.execute(
        "UPDATE offers SET status=?, followup1_due=?, followup2_due=? WHERE offer_id=?",
        (status, fu1, fu2, offer_id),
    )
    con.commit()
    con.close()


def due_followups(today: dt.date | None = None) -> list[sqlite3.Row]:
    con = connect()
    cur = con.cursor()
    t = (today or dt.date.today()).isoformat()
    cur.execute(
        """
        SELECT * FROM offers
        WHERE status='applied' AND (
            followup1_due = ? OR followup2_due = ?
        )
        ORDER BY score DESC
        """,
        (t, t),
    )
    rows = cur.fetchall()
    con.close()
    return rows


def recent_new_offers(limit: int = 50) -> list[sqlite3.Row]:
    con = connect()
    cur = con.cursor()
    cur.execute(
        """
        SELECT * FROM offers
        WHERE status='new' AND (last_notified_at IS NULL OR last_notified_at = '')
        ORDER BY inserted_at DESC, score DESC
        LIMIT ?
        """,
        (limit,),
    )
    rows = cur.fetchall()
    con.close()
    return rows


def mark_notified(offer_ids: list[str]) -> None:
    if not offer_ids:
        return
    con = connect()
    cur = con.cursor()
    now = dt.datetime.utcnow().isoformat()
    cur.executemany(
        "UPDATE offers SET last_notified_at=? WHERE offer_id=?",
        [(now, oid) for oid in offer_ids],
    )
    con.commit()
    con.close()


def query_offers(
    *,
    days: int | None = None,
    from_date: str | None = None,
    to_date: str | None = None,
    status: str | None = None,
    min_score: float | None = None,
    limit: int = 100,
    order_by: str = "score_desc",
) -> list[sqlite3.Row]:
    con = connect()
    cur = con.cursor()
    where = []
    params: list[Any] = []

    # We filter on inserted_at (UTC ISO string) for recency; can be swapped for published_at if desired.
    if days is not None:
        start = (dt.datetime.utcnow() - dt.timedelta(days=days)).isoformat()
        where.append("inserted_at >= ?")
        params.append(start)
    if from_date:
        where.append("inserted_at >= ?")
        params.append(from_date)
    if to_date:
        # add one day to include the whole day when only a date is provided
        if len(to_date) == 10:
            to_dt = dt.datetime.fromisoformat(to_date) + dt.timedelta(days=1)
            where.append("inserted_at < ?")
            params.append(to_dt.isoformat())
        else:
            where.append("inserted_at <= ?")
            params.append(to_date)
    if status:
        where.append("status = ?")
        params.append(status)
    if min_score is not None:
        where.append("score >= ?")
        params.append(float(min_score))

    order_sql = {
        "score_desc": "score DESC, inserted_at DESC",
        "date_desc": "inserted_at DESC, score DESC",
    }.get(order_by, "score DESC, inserted_at DESC")

    sql = "SELECT * FROM offers"
    if where:
        sql += " WHERE " + " AND ".join(where)
    sql += f" ORDER BY {order_sql} LIMIT ?"
    params.append(int(limit))
    cur.execute(sql, params)
    rows = cur.fetchall()
    con.close()
    return rows


def update_offer_details(offer_id: str, fields: dict[str, Any]) -> None:
    if not fields:
        return
    con = connect()
    cur = con.cursor()
    cols = []
    params: list[Any] = []
    for k, v in fields.items():
        cols.append(f"{k}=?")
        params.append(v)
    params.append(offer_id)
    sql = f"UPDATE offers SET {', '.join(cols)} WHERE offer_id=?"
    cur.execute(sql, params)
    con.commit()
    con.close()


--- END OF FILE src/ft_job_alerts/storage.py ---

